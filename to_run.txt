

---- Comparison between HSIC and MI:

-- pretrain:

CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=2 pretrain.py --batch_size 128 --model mae_vit_ti_cifar10 --norm_pix_loss --mask_ratio 0.75 --epochs 800 --warmup_epochs 40 --blr 1.5e-4 --weight_decay 0.05 --dataset cifar10 --output_dir ./experiment/mae_cifar10_adv_fast_hsicpretrain/ --log_dir ./experiment/mae_cifar10_adv_fast_hsicpretrain/ --attack pgd_mae --steps 1 --alpha 0.0392 --num_workers 16 --mi_train hsic --mi_xpl 0.00001 > experiment/mae_cifar10_adv_fast_hsicpretrain/printlog 2>&1

-- fine-tune:
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 finetune.py --finetune experiment/mae_cifar10_adv_fast_hsicpretrain1/checkpoint-799.pth --model vit_ti_patch2_cifar10 --output_dir experiment/mae_cifar10_advfinetune_with_adv_fast_hsicpretrain1/ --log_dir experiment/mae_cifar10_advfinetune_with_adv_fast_hsicpretrain1/ --batch_size 16 --epochs 200 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --reprob 0.25 --dataset cifar10 --nb_classes 10 --attack_train pgd --mi_loss dib_mi > experiment/mae_cifar10_advfinetune_with_adv_fast_hsicpretrain1/printlog 2>&1 &

use '--num_workers 32' at leiden GPUs 


--mixup 0.8 --cutmix 1.0 


---- evaluation our method, comparison with sota methods on multiple model architectures.


ImageNet Pretrain:
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=8 pretrain.py --batch_size 256 --model mae_vit_ti_patch16 --norm_pix_loss --mask_ratio 0.75 --epochs 800 --warmup_epochs 40 --blr 1.5e-4 --weight_decay 0.05 --dataset imagenet --data_root ../data/imagenet --input_size 224 --output_dir ./experiment/mae_imagenet_adv_fast_dibpretrain/ --log_dir ./experiment/mae_imagenet_adv_fast_dibpretrain/ --attack pgd_mae --steps 1 --alpha 0.0392 --num_workers 0 --mi_train dib --mi_xpl 0.00001 > experiment/mae_imagenet_adv_fast_dibpretrain/printlog 2>&1 &

ImageNet Fine-tune:
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 finetune.py --finetune experiment/mae_cifar10_adv_fast_hsicpretrain1/checkpoint-799.pth --model vit_ti_patch2_cifar10 --output_dir experiment/mae_imagenet_advfinetune_with_adv_fast_dibpretrain/ --log_dir experiment/mae_imagenet_advfinetune_with_adv_fast_dibpretrain/ --batch_size 16 --epochs 50 --blr 5e-4 --layer_decay 0.65 --weight_decay 0.05 --drop_path 0.1 --mixup 0.8 --cutmix 1.0 --reprob 0.25 --dataset cifar10 --nb_classes 10 --attack_train pgd > experiment/mae_imagenet_advfinetune_with_adv_fast_dibpretrain/printlog 2>&1 &

